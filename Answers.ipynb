{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔍 Murder in the Machine Learning Manor 🔎\n",
        "\n",
        "## A Data Science Detective Investigation\n",
        "\n",
        "![Crime Scene](data/assets/1.png)\n",
        "\n",
        "### 📱 BREAKING NEWS 📱\n",
        "\n",
        "**TRAGEDY AT MACHINE LEARNING MANOR**: Renowned data scientist Professor Reginald \"Regressor\" Fisher has been found dead in his study during the annual International Conference on Statistical Learning. The cause of death appears to be blunt force trauma from what investigators believe to be a vintage calculating machine.\n",
        "\n",
        "**Detective's Note**: _You've been called in as data science detectives to solve this case using your machine learning expertise. Eight suspects were at the manor during the time of the murder. Each has motives, alibis, and various characteristics that may point to their guilt or innocence. Your job is to analyze the evidence and identify the killer using the techniques you've learned in class._\n",
        "\n",
        "**Your Task**: Progress through this notebook, analyzing the evidence, and building different models to identify the killer. You'll discover that some models may struggle with certain evidence patterns, while others might just crack the case!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case Setup\n",
        "\n",
        "First, let's import the necessary detective tools (libraries) and examine the evidence (data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import our detective tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import modeling libraries\n",
        "# For modeling - import what you need\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Set aesthetic style of the plots\n",
        "sns.set_style('darkgrid')\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "np.random.seed(42)  # The answer to everything (DO NOT MODIFY THIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case Background\n",
        "\n",
        "**Detective's Note**: _You have access to two crucial datasets:_\n",
        "\n",
        "1. **Previous Case Files** (`previous_murders_training_data.csv`): Records from previous solved cases with known guilt scores.\n",
        "2. **Current Case Evidence** (`current_murder_evidence.csv`): Evidence collected from the current murder investigation without known guilt scores.\n",
        "\n",
        "_Your mission is to analyze patterns from previous cases to determine who is most likely guilty in the current case._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Examining Previous Cases\n",
        "\n",
        "![Evidence Locker](data/assets/2.png)\n",
        "\n",
        "**Detective's Note**: _Let's first examine the records from previous cases to understand what factors are associated with guilt._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the previous case files\n",
        "previous_cases_file = 'data/previous_murders_training_data.csv'\n",
        "previous_cases = pd.read_csv(previous_cases_file)\n",
        "\n",
        "# Display the first few rows to understand the data structure\n",
        "previous_cases.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Detective's Note**: _These previous cases contain a 'guilt_score' column which indicates how likely each suspect was to have committed the crime (higher values = more likely to be guilty). The other columns represent evidence, characteristics, and circumstances surrounding each suspect._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the structure of the previous cases\n",
        "# 1. Print the dataset shape\n",
        "print(f\"Dataset shape: {previous_cases.shape}\")\n",
        "\n",
        "# 2. Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(previous_cases.isnull().sum())\n",
        "\n",
        "# 3. Examine the distribution of guilt scores\n",
        "print(\"\\nGuilt score statistics:\")\n",
        "print(previous_cases['guilt_score'].describe())\n",
        "\n",
        "# Plot the distribution of guilt scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(previous_cases['guilt_score'], bins=30, kde=True)\n",
        "plt.title('Distribution of Guilt Scores in Previous Cases')\n",
        "plt.xlabel('Guilt Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine if there are any extremely high guilt scores \n",
        "# This might indicate \"smoking gun\" evidence patterns\n",
        "high_guilt = previous_cases[previous_cases['guilt_score'] > 0.85]\n",
        "print(f\"Number of high guilt cases (>0.85): {len(high_guilt)}\")\n",
        "\n",
        "if len(high_guilt) > 0:\n",
        "    print(\"\\nExample of high guilt cases:\")\n",
        "    print(high_guilt.head())\n",
        "    \n",
        "    # Let's see what features these high guilt cases have in common\n",
        "    print(\"\\nFeature statistics for high guilt cases:\")\n",
        "    numeric_high_guilt = high_guilt.select_dtypes(include=['number'])\n",
        "    print(numeric_high_guilt.describe().round(2).T[['mean', 'min', 'max']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a heatmap to visualize the correlation between features and guilt\n",
        "# 1. Select the numeric columns\n",
        "numeric_columns = previous_cases.select_dtypes(include=['number']).columns\n",
        "\n",
        "# 2. Calculate the correlation matrix\n",
        "correlation_matrix = previous_cases[numeric_columns].corr()\n",
        "\n",
        "# 3. Create a heatmap visualization\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Let's also look at the top correlations with guilt_score specifically\n",
        "guilt_correlations = correlation_matrix['guilt_score'].drop('guilt_score').sort_values(ascending=False)\n",
        "print(\"\\nTop features correlated with guilt:\")\n",
        "print(guilt_correlations.head(10))\n",
        "\n",
        "print(\"\\nFeatures negatively correlated with guilt:\")\n",
        "print(guilt_correlations.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Detective's Note**: _Interestingly, while the average correlations can be informative, they might not tell the whole story. In many criminal cases, a single piece of damning evidence (a \"smoking gun\") can be more important than many weak correlations. Let's keep this in mind as we build our models._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group Discussion (5 minutes)**: \n",
        "- What factors appear to be correlated with guilt in previous cases?\n",
        "- Are there any surprising relationships in the data?\n",
        "- What evidence would you prioritize if you were investigating a new case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Building Detective Models\n",
        "\n",
        "![Detective at Desk](data/assets/4.png)\n",
        "\n",
        "**Detective's Note**: _Now that we understand the previous cases, let's build different detective models to learn patterns of guilt. Each model has its own approach to analyzing evidence._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare previous cases data for modeling\n",
        "# 1. Separate features (X) and target (y = guilt_score)\n",
        "# First, drop non-predictive columns\n",
        "X = previous_cases.drop(['suspect_id', 'suspect_name', 'guilt_score'], axis=1)\n",
        "y = previous_cases['guilt_score']\n",
        "\n",
        "# 2. Handle categorical variables using one-hot encoding\n",
        "X = pd.get_dummies(X, columns=['relationship_to_victim'], drop_first=False)\n",
        "\n",
        "# 3. Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Standardize numerical features\n",
        "# First identify numeric columns (excluding binary/one-hot encoded features)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "binary_cols = [col for col in X.columns if col.startswith('relationship_to_victim_')]\n",
        "numeric_cols = [col for col in numeric_cols if col not in binary_cols]\n",
        "\n",
        "# Create scaler and fit on training data only\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_val_scaled = X_val.copy()\n",
        "\n",
        "# Apply scaling to numeric columns only\n",
        "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_val_scaled[numeric_cols] = scaler.transform(X_val[numeric_cols])\n",
        "\n",
        "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
        "print(f\"Validation features shape: {X_val_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detective Model 1: Linear Regression\n",
        "\n",
        "**Detective's Note**: _This model analyzes evidence by looking at the overall relationships between each piece of evidence and guilt. It treats all data points equally and focuses on average patterns rather than specific combinations of evidence._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a linear regression model on previous cases\n",
        "# 1. Create and fit a linear regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 2. Evaluate model performance (R²)\n",
        "train_r2 = linear_model.score(X_train_scaled, y_train)\n",
        "val_r2 = linear_model.score(X_val_scaled, y_val)\n",
        "\n",
        "print(f\"Linear Regression Model - Training R²: {train_r2:.4f}\")\n",
        "print(f\"Linear Regression Model - Validation R²: {val_r2:.4f}\")\n",
        "\n",
        "# Make predictions and calculate RMSE\n",
        "y_val_pred = linear_model.predict(X_val_scaled)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(f\"Linear Regression Model - Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "# 3. Examine coefficients to see what evidence this model values\n",
        "# Create a DataFrame with feature names and their coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X_train_scaled.columns,\n",
        "    'Coefficient': linear_model.coef_\n",
        "})\n",
        "\n",
        "# Sort by absolute value of coefficient (most important features first)\n",
        "coefficients['Abs_Coefficient'] = np.abs(coefficients['Coefficient'])\n",
        "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the top 10 most important features according to linear regression\n",
        "print(\"\\nTop 10 most important features according to Linear Regression:\")\n",
        "print(coefficients.head(10))\n",
        "\n",
        "# Plot the top 15 features by coefficient magnitude\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = coefficients.head(15)\n",
        "colors = ['green' if c > 0 else 'red' for c in top_features['Coefficient']]\n",
        "sns.barplot(x='Coefficient', y='Feature', data=top_features, palette=colors)\n",
        "plt.title('Top 15 Feature Importance in Linear Regression Model')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detective Model 2: Decision Tree\n",
        "\n",
        "**Detective's Note**: _This model works like a detective asking a series of yes/no questions about the evidence to determine guilt. It can find patterns in specific combinations of evidence that might be missed by linear models._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a decision tree regression model\n",
        "# 1. Create and fit a decision tree regressor with increased depth to capture complex patterns\n",
        "tree_model = DecisionTreeRegressor(max_depth=8, random_state=42)\n",
        "tree_model.fit(X_train, y_train)  # Note: Trees don't require scaling\n",
        "\n",
        "# 2. Evaluate model performance (R²)\n",
        "tree_train_r2 = tree_model.score(X_train, y_train)\n",
        "tree_val_r2 = tree_model.score(X_val, y_val)\n",
        "\n",
        "print(f\"Decision Tree Model - Training R²: {tree_train_r2:.4f}\")\n",
        "print(f\"Decision Tree Model - Validation R²: {tree_val_r2:.4f}\")\n",
        "\n",
        "# Make predictions and calculate RMSE\n",
        "y_val_pred_tree = tree_model.predict(X_val)\n",
        "tree_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_tree))\n",
        "print(f\"Decision Tree Model - Validation RMSE: {tree_val_rmse:.4f}\")\n",
        "\n",
        "# 3. Extract and visualize feature importance\n",
        "tree_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': tree_model.feature_importances_\n",
        "})\n",
        "tree_importances = tree_importances.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the top 10 most important features according to the decision tree\n",
        "print(\"\\nTop 10 most important features according to Decision Tree:\")\n",
        "print(tree_importances.head(10))\n",
        "\n",
        "# Plot the top 15 features by importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_tree_features = tree_importances.head(15)\n",
        "sns.barplot(x='Importance', y='Feature', data=top_tree_features, color='skyblue')\n",
        "plt.title('Top 15 Feature Importance in Decision Tree Model')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize the tree structure (first few levels)\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_model, feature_names=list(X_train.columns), filled=True, max_depth=3, fontsize=10)\n",
        "plt.title('Decision Tree Structure (First 3 Levels)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detective Model 3: Random Forest\n",
        "\n",
        "**Detective's Note**: _This model is like a team of detectives, each examining the evidence from a slightly different angle, then coming together to make a final determination. Random Forests are especially good at identifying specific patterns or \"smoking gun\" evidence that might be buried in a sea of other information._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a random forest regression model\n",
        "# 1. Create and fit a random forest regressor with increased complexity\n",
        "forest_model = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_leaf=2, random_state=42)\n",
        "forest_model.fit(X_train, y_train)  # Random Forests don't require scaling\n",
        "\n",
        "# 2. Evaluate model performance (R²)\n",
        "forest_train_r2 = forest_model.score(X_train, y_train)\n",
        "forest_val_r2 = forest_model.score(X_val, y_val)\n",
        "\n",
        "print(f\"Random Forest Model - Training R²: {forest_train_r2:.4f}\")\n",
        "print(f\"Random Forest Model - Validation R²: {forest_val_r2:.4f}\")\n",
        "\n",
        "# Make predictions and calculate RMSE\n",
        "y_val_pred_forest = forest_model.predict(X_val)\n",
        "forest_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_forest))\n",
        "print(f\"Random Forest Model - Validation RMSE: {forest_val_rmse:.4f}\")\n",
        "\n",
        "# 3. Extract and visualize feature importance\n",
        "forest_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': forest_model.feature_importances_\n",
        "})\n",
        "forest_importances = forest_importances.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the top 10 most important features according to the random forest\n",
        "print(\"\\nTop 10 most important features according to Random Forest:\")\n",
        "print(forest_importances.head(10))\n",
        "\n",
        "# Plot the top 15 features by importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_forest_features = forest_importances.head(15)\n",
        "sns.barplot(x='Importance', y='Feature', data=top_forest_features, color='green')\n",
        "plt.title('Top 15 Feature Importance in Random Forest Model')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's check how our models handle high-guilt cases\n",
        "![Detective at Desk](data/assets/3.png)\n",
        "**Detective's Note**: _We should specifically look at how well each model performs at identifying very high guilt cases. These would be the \"smoking gun\" patterns that we want to detect in our current case._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check how our models perform on the very high guilt cases\n",
        "high_guilt_indices = y_val.index[y_val > 0.85]\n",
        "\n",
        "if len(high_guilt_indices) > 0:\n",
        "    high_guilt_X = X_val.loc[high_guilt_indices]\n",
        "    high_guilt_X_scaled = X_val_scaled.loc[high_guilt_indices]\n",
        "    high_guilt_y = y_val.loc[high_guilt_indices]\n",
        "    \n",
        "    # Make predictions\n",
        "    linear_high_guilt_pred = linear_model.predict(high_guilt_X_scaled)\n",
        "    tree_high_guilt_pred = tree_model.predict(high_guilt_X)\n",
        "    forest_high_guilt_pred = forest_model.predict(high_guilt_X)\n",
        "    \n",
        "    # Compare results\n",
        "    print(\"Performance on high guilt cases (guilt > 0.85):\")\n",
        "    print(f\"Linear regression average prediction: {np.mean(linear_high_guilt_pred):.4f}\")\n",
        "    print(f\"Decision tree average prediction: {np.mean(tree_high_guilt_pred):.4f}\")\n",
        "    print(f\"Random forest average prediction: {np.mean(forest_high_guilt_pred):.4f}\")\n",
        "    print(f\"Actual average guilt: {np.mean(high_guilt_y):.4f}\")\n",
        "    \n",
        "    # Plot individual predictions\n",
        "    results = pd.DataFrame({\n",
        "        'Actual': high_guilt_y,\n",
        "        'Linear': linear_high_guilt_pred,\n",
        "        'Tree': tree_high_guilt_pred,\n",
        "        'Forest': forest_high_guilt_pred\n",
        "    })\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    results.plot(kind='bar', figsize=(12, 6))\n",
        "    plt.title('Model Predictions on High Guilt Cases')\n",
        "    plt.ylabel('Guilt Score')\n",
        "    plt.grid(axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No high guilt cases (guilt > 0.85) found in the validation set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performances\n",
        "# 1. Create a comparison visualization of all three model performances\n",
        "models = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
        "train_scores = [train_r2, tree_train_r2, forest_train_r2]\n",
        "val_scores = [val_r2, tree_val_r2, forest_val_r2]\n",
        "rmse_scores = [val_rmse, tree_val_rmse, forest_val_rmse]\n",
        "\n",
        "# Plot R² comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "X_axis = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(X_axis - width/2, train_scores, width, label='Training R²')\n",
        "plt.bar(X_axis + width/2, val_scores, width, label='Validation R²')\n",
        "\n",
        "plt.xticks(X_axis, models)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Model Performance Comparison (R²)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot RMSE comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, rmse_scores, color='coral')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('RMSE (lower is better)')\n",
        "plt.title('Model Performance Comparison (RMSE)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Let's compare which features each model considers most important\n",
        "# First, standardize the importance scores\n",
        "linear_importance = coefficients.copy()\n",
        "linear_importance['Normalized_Importance'] = linear_importance['Abs_Coefficient'] / linear_importance['Abs_Coefficient'].max()\n",
        "\n",
        "tree_importances['Normalized_Importance'] = tree_importances['Importance'] / tree_importances['Importance'].max()\n",
        "forest_importances['Normalized_Importance'] = forest_importances['Importance'] / forest_importances['Importance'].max()\n",
        "\n",
        "# Identify top 5 features for each model\n",
        "print(\"Top 5 features by model:\")\n",
        "print(\"\\nLinear Regression:\")\n",
        "print(linear_importance[['Feature', 'Coefficient']].head(5))\n",
        "\n",
        "print(\"\\nDecision Tree:\")\n",
        "print(tree_importances[['Feature', 'Importance']].head(5))\n",
        "\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(forest_importances[['Feature', 'Importance']].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group Discussion (10 minutes)**:\n",
        "- Which model performed best on previous cases?\n",
        "- What evidence did each model consider most important?\n",
        "- Why might different models value different types of evidence?\n",
        "- What kinds of evidence patterns might tree-based models detect that linear models cannot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Investigating the Current Case\n",
        "\n",
        "**Detective's Note**: _Now it's time to apply our trained detective models to the current murder case. Let's load the evidence and see who each model identifies as the most likely culprit._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the current case evidence\n",
        "current_case_file = 'data/current_murder_evidence.csv'\n",
        "current_evidence = pd.read_csv(current_case_file)\n",
        "\n",
        "# Display the first few rows\n",
        "current_evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the structure of the current evidence\n",
        "# 1. Check the dataset shape\n",
        "print(f\"Current evidence dataset shape: {current_evidence.shape}\")\n",
        "\n",
        "# 2. Identify the suspects in this case\n",
        "suspects = current_evidence['suspect_name'].unique()\n",
        "print(f\"\\nSuspects in the current case ({len(suspects)}):\")\n",
        "for i, suspect in enumerate(suspects, 1):\n",
        "    print(f\"{i}. {suspect}\")\n",
        "\n",
        "# Count data points per suspect\n",
        "suspect_counts = current_evidence['suspect_name'].value_counts()\n",
        "print(\"\\nNumber of evidence points per suspect:\")\n",
        "print(suspect_counts)\n",
        "\n",
        "# 3. Confirm there's no guilt_score column\n",
        "print(\"\\nNote: As expected, there is no 'guilt_score' column in the current evidence.\")\n",
        "print(\"We'll need to use our models to predict it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the current case data for prediction\n",
        "# 1. Apply the same preprocessing steps used on the previous cases\n",
        "# First, separate the suspect identification columns\n",
        "suspect_info = current_evidence[['suspect_id', 'suspect_name']]\n",
        "X_current = current_evidence.drop(['suspect_id', 'suspect_name'], axis=1)\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"Missing values in current evidence:\")\n",
        "print(X_current.isnull().sum().sum())\n",
        "\n",
        "if X_current.isnull().sum().sum() > 0:\n",
        "    print(\"Columns with missing values:\")\n",
        "    print(X_current.columns[X_current.isnull().any()].tolist())\n",
        "    \n",
        "    # Fill missing values with appropriate strategies\n",
        "    # For this case, we'll use median for numeric columns and mode for categorical\n",
        "    for col in X_current.columns:\n",
        "        if X_current[col].isnull().any():\n",
        "            if X_current[col].dtype.kind in 'f':\n",
        "                # Numeric column - use median\n",
        "                X_current[col] = X_current[col].fillna(X_current[col].median())\n",
        "            else:\n",
        "                # Categorical column - use mode\n",
        "                X_current[col] = X_current[col].fillna(X_current[col].mode()[0])\n",
        "\n",
        "# 2. Handle categorical variables with one-hot encoding\n",
        "# Get the same columns as in training\n",
        "X_current_encoded = pd.get_dummies(X_current, columns=['relationship_to_victim'], drop_first=False)\n",
        "\n",
        "# 3. Ensure feature columns match those used in training\n",
        "# Get the list of columns from the training data\n",
        "training_columns = X_train.columns.tolist()\n",
        "\n",
        "# Find columns in current data that weren't in training data\n",
        "new_columns = [col for col in X_current_encoded.columns if col not in training_columns]\n",
        "if new_columns:\n",
        "    print(f\"New columns found in current evidence: {new_columns}\")\n",
        "    # Drop columns that weren't in training data\n",
        "    X_current_encoded = X_current_encoded.drop(columns=new_columns)\n",
        "\n",
        "# Add missing columns that were in training data but not in current data\n",
        "missing_columns = [col for col in training_columns if col not in X_current_encoded.columns]\n",
        "if missing_columns:\n",
        "    print(f\"Missing columns in current evidence: {missing_columns}\")\n",
        "    # Add missing columns with zeros\n",
        "    for col in missing_columns:\n",
        "        X_current_encoded[col] = 0\n",
        "\n",
        "# Ensure columns are in the same order as in training\n",
        "X_current_encoded = X_current_encoded[training_columns]\n",
        "\n",
        "# 4. Apply the same scaling to numeric features\n",
        "X_current_scaled = X_current_encoded.copy()\n",
        "X_current_scaled[numeric_cols] = scaler.transform(X_current_encoded[numeric_cols])\n",
        "\n",
        "print(f\"Current evidence features shape: {X_current_encoded.shape}\")\n",
        "print(f\"Original training features shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply each model to predict guilt scores for the current case\n",
        "# 1. Use each trained model to predict guilt scores\n",
        "linear_predictions = linear_model.predict(X_current_scaled)\n",
        "tree_predictions = tree_model.predict(X_current_encoded)\n",
        "forest_predictions = forest_model.predict(X_current_encoded)\n",
        "\n",
        "# 2. Add these predictions to the evidence dataframe\n",
        "results = pd.DataFrame({\n",
        "    'suspect_id': suspect_info['suspect_id'],\n",
        "    'suspect_name': suspect_info['suspect_name'],\n",
        "    'linear_guilt': linear_predictions,\n",
        "    'tree_guilt': tree_predictions,\n",
        "    'forest_guilt': forest_predictions\n",
        "})\n",
        "\n",
        "# Display a few predictions\n",
        "print(\"Sample of predictions:\")\n",
        "results.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Solving the Case\n",
        "\n",
        "![Case Solved](data/assets/5.png)\n",
        "\n",
        "**Detective's Note**: _Let's analyze both the average and maximum guilt scores predicted by each model for each suspect. We need to be especially vigilant for potential \"smoking gun\" evidence that might only appear in a few data points._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate average predicted guilt for each suspect by each model\n",
        "# 1. Group by suspect_name and compute mean for each model's predictions\n",
        "avg_guilt = results.groupby('suspect_name').mean().reset_index()\n",
        "\n",
        "# Round to 4 decimal places for readability\n",
        "avg_guilt['linear_guilt'] = avg_guilt['linear_guilt'].round(4)\n",
        "avg_guilt['tree_guilt'] = avg_guilt['tree_guilt'].round(4)\n",
        "avg_guilt['forest_guilt'] = avg_guilt['forest_guilt'].round(4)\n",
        "\n",
        "# 2. Create a comparison table showing each model's top suspects by average guilt\n",
        "print(\"Average guilt scores by model:\")\n",
        "print(avg_guilt[['suspect_name', 'linear_guilt', 'tree_guilt', 'forest_guilt']])\n",
        "\n",
        "# Create separate DataFrames for each model's average guilt rankings\n",
        "linear_ranking = avg_guilt[['suspect_name', 'linear_guilt']].sort_values('linear_guilt', ascending=False).reset_index(drop=True)\n",
        "tree_ranking = avg_guilt[['suspect_name', 'tree_guilt']].sort_values('tree_guilt', ascending=False).reset_index(drop=True)\n",
        "forest_ranking = avg_guilt[['suspect_name', 'forest_guilt']].sort_values('forest_guilt', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display each model's suspect rankings by average guilt\n",
        "print(\"\\nLinear Regression Model - Suspect Rankings (Average Guilt):\")\n",
        "print(linear_ranking)\n",
        "\n",
        "print(\"\\nDecision Tree Model - Suspect Rankings (Average Guilt):\")\n",
        "print(tree_ranking)\n",
        "\n",
        "print(\"\\nRandom Forest Model - Suspect Rankings (Average Guilt):\")\n",
        "print(forest_ranking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now calculate MAXIMUM guilt score for each suspect to detect smoking gun evidence\n",
        "max_guilt = results.groupby('suspect_name').max().reset_index()\n",
        "\n",
        "# Round to 4 decimal places for readability\n",
        "max_guilt['linear_guilt'] = max_guilt['linear_guilt'].round(4)\n",
        "max_guilt['tree_guilt'] = max_guilt['tree_guilt'].round(4)\n",
        "max_guilt['forest_guilt'] = max_guilt['forest_guilt'].round(4)\n",
        "\n",
        "# Create a comparison table showing each model's top suspects by maximum guilt\n",
        "print(\"Maximum guilt scores by model:\")\n",
        "print(max_guilt[['suspect_name', 'linear_guilt', 'tree_guilt', 'forest_guilt']])\n",
        "\n",
        "# Create separate DataFrames for each model's maximum guilt rankings\n",
        "max_linear_ranking = max_guilt[['suspect_name', 'linear_guilt']].sort_values('linear_guilt', ascending=False).reset_index(drop=True)\n",
        "max_tree_ranking = max_guilt[['suspect_name', 'tree_guilt']].sort_values('tree_guilt', ascending=False).reset_index(drop=True)\n",
        "max_forest_ranking = max_guilt[['suspect_name', 'forest_guilt']].sort_values('forest_guilt', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display each model's suspect rankings by maximum guilt\n",
        "print(\"\\nLinear Regression Model - Suspect Rankings (Maximum Guilt):\")\n",
        "print(max_linear_ranking)\n",
        "\n",
        "print(\"\\nDecision Tree Model - Suspect Rankings (Maximum Guilt):\")\n",
        "print(max_tree_ranking)\n",
        "\n",
        "print(\"\\nRandom Forest Model - Suspect Rankings (Maximum Guilt):\")\n",
        "print(max_forest_ranking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find any suspect with extremely high guilt scores in any data point (potential smoking gun)\n",
        "high_guilt_threshold = 0.75\n",
        "high_guilt_points = results[results['forest_guilt'] > high_guilt_threshold]\n",
        "\n",
        "if not high_guilt_points.empty:\n",
        "    print(f\"Found {len(high_guilt_points)} data points with guilt scores above {high_guilt_threshold}\")\n",
        "    print(high_guilt_points[['suspect_name', 'linear_guilt', 'tree_guilt', 'forest_guilt']].sort_values('forest_guilt', ascending=False))\n",
        "    \n",
        "    # Let's examine the smoking gun evidence for the top suspect\n",
        "    top_high_guilt_idx = high_guilt_points['forest_guilt'].idxmax()\n",
        "    smoking_gun_evidence = current_evidence.loc[top_high_guilt_idx]\n",
        "    \n",
        "    print(f\"\\nSmoking gun evidence for {smoking_gun_evidence['suspect_name']}:\")\n",
        "    \n",
        "    # Show the key features that might be contributing to this high guilt score\n",
        "    important_features = ['alibi_strength', 'motive_strength', 'prior_conflict', 'fingerprints_at_scene',\n",
        "                         'dna_match_strength', 'at_scene_during_murder', 'had_opportunity',\n",
        "                         'suspicious_behavior', 'inconsistent_statements']\n",
        "    \n",
        "    for feature in important_features:\n",
        "        if feature in smoking_gun_evidence:\n",
        "            print(f\"{feature}: {smoking_gun_evidence[feature]}\")\n",
        "else:\n",
        "    print(f\"No data points found with guilt scores above {high_guilt_threshold}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a visualization comparing the suspects across models (average guilt)\n",
        "# 1. Bar chart showing average guilt scores by model and suspect\n",
        "# Prepare data for plotting\n",
        "plot_data = pd.melt(avg_guilt,\n",
        "                   id_vars=['suspect_name'], \n",
        "                   value_vars=['linear_guilt', 'tree_guilt', 'forest_guilt'],\n",
        "                   var_name='Model', \n",
        "                   value_name='Average Guilt Score')\n",
        "\n",
        "# Map model names to more readable labels\n",
        "plot_data['Model'] = plot_data['Model'].map({\n",
        "    'linear_guilt': 'Linear Regression',\n",
        "    'tree_guilt': 'Decision Tree',\n",
        "    'forest_guilt': 'Random Forest'\n",
        "})\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='suspect_name', y='Average Guilt Score', hue='Model', data=plot_data)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Average Predicted Guilt Scores by Model and Suspect')\n",
        "plt.xlabel('Suspect')\n",
        "plt.ylabel('Average Guilt Score')\n",
        "plt.legend(title='Model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a visualization comparing the suspects across models (maximum guilt)\n",
        "# Prepare data for plotting maximum guilt\n",
        "max_plot_data = pd.melt(max_guilt,\n",
        "                       id_vars=['suspect_name'], \n",
        "                       value_vars=['linear_guilt', 'tree_guilt', 'forest_guilt'],\n",
        "                       var_name='Model', \n",
        "                       value_name='Maximum Guilt Score')\n",
        "\n",
        "# Map model names to more readable labels\n",
        "max_plot_data['Model'] = max_plot_data['Model'].map({\n",
        "    'linear_guilt': 'Linear Regression',\n",
        "    'tree_guilt': 'Decision Tree',\n",
        "    'forest_guilt': 'Random Forest'\n",
        "})\n",
        "\n",
        "# Create the bar chart for maximum guilt\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='suspect_name', y='Maximum Guilt Score', hue='Model', data=max_plot_data)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Maximum Predicted Guilt Scores by Model and Suspect')\n",
        "plt.xlabel('Suspect')\n",
        "plt.ylabel('Maximum Guilt Score')\n",
        "plt.legend(title='Model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create heatmaps for both average and maximum guilt\n",
        "# 1. Average guilt heatmap\n",
        "guilt_pivot_avg = avg_guilt.set_index('suspect_name')[['linear_guilt', 'tree_guilt', 'forest_guilt']]\n",
        "guilt_pivot_avg.columns = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.heatmap(guilt_pivot_avg, annot=True, cmap='YlOrRd', linewidths=0.5, fmt='.4f')\n",
        "plt.title('Average Guilt Score Heatmap by Model and Suspect')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Maximum guilt heatmap\n",
        "guilt_pivot_max = max_guilt.set_index('suspect_name')[['linear_guilt', 'tree_guilt', 'forest_guilt']]\n",
        "guilt_pivot_max.columns = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.heatmap(guilt_pivot_max, annot=True, cmap='YlOrRd', linewidths=0.5, fmt='.4f')\n",
        "plt.title('Maximum Guilt Score Heatmap by Model and Suspect')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Detective's Note**: _The MAXIMUM guilt scores reveal something very interesting! While the average guilt would point to one suspect, looking at the specific pattern or \"smoking gun\" evidence points to another suspect entirely. Tree-based models are particularly good at identifying these specific patterns._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze evidence patterns for the top suspects by both average and maximum guilt\n",
        "# Get top suspect from each approach\n",
        "avg_top_suspect = forest_ranking.iloc[0]['suspect_name']\n",
        "max_top_suspect = max_forest_ranking.iloc[0]['suspect_name']\n",
        "\n",
        "print(f\"Top suspect by average guilt (Random Forest): {avg_top_suspect}\")\n",
        "print(f\"Top suspect by maximum guilt (Random Forest): {max_top_suspect}\")\n",
        "\n",
        "# Let's get the evidence for these top suspects\n",
        "top_suspects = set([avg_top_suspect, max_top_suspect])\n",
        "\n",
        "for suspect in top_suspects:\n",
        "    suspect_data = current_evidence[current_evidence['suspect_name'] == suspect]\n",
        "    suspect_results = results[results['suspect_name'] == suspect]\n",
        "    \n",
        "    print(f\"\\nEvidence summary for {suspect}:\")\n",
        "    numeric_evidence = suspect_data.select_dtypes(include=['number'])\n",
        "    print(numeric_evidence.describe().T[['mean', 'min', 'max']])\n",
        "    \n",
        "    # Calculate the percentage of times certain key binary flags are True\n",
        "    binary_cols = ['at_scene_during_murder', 'had_opportunity', 'fingerprints_at_scene', 'prior_conflict']\n",
        "    binary_percent = {}\n",
        "    for col in binary_cols:\n",
        "        if col in suspect_data.columns:\n",
        "            binary_percent[col] = suspect_data[col].mean() * 100\n",
        "    \n",
        "    print(\"\\nBinary evidence percentages:\")\n",
        "    for col, percentage in binary_percent.items():\n",
        "        print(f\"{col}: {percentage:.1f}%\")\n",
        "        \n",
        "    # Check relationship to victim\n",
        "    relationship_counts = suspect_data['relationship_to_victim'].value_counts(normalize=True) * 100\n",
        "    print(\"\\nRelationship to victim percentages:\")\n",
        "    for relationship, percentage in relationship_counts.items():\n",
        "        print(f\"{relationship}: {percentage:.1f}%\")\n",
        "        \n",
        "    # Find maximum guilt score for each model\n",
        "    max_linear = suspect_results['linear_guilt'].max()\n",
        "    max_tree = suspect_results['tree_guilt'].max()\n",
        "    max_forest = suspect_results['forest_guilt'].max()\n",
        "    \n",
        "    print(f\"\\n{suspect} - Maximum guilt scores:\")\n",
        "    print(f\"Linear Regression: {max_linear:.4f}\")\n",
        "    print(f\"Decision Tree: {max_tree:.4f}\")\n",
        "    print(f\"Random Forest: {max_forest:.4f}\")\n",
        "    \n",
        "    # If this is the smoking gun suspect, let's examine the specific evidence point\n",
        "    if max_forest > 0.8:  # Only look at very high guilt scores\n",
        "        smoking_gun_idx = suspect_results['forest_guilt'].idxmax()\n",
        "        smoking_gun = current_evidence.loc[smoking_gun_idx]\n",
        "        \n",
        "        print(f\"\\nSmoking gun evidence for {suspect}:\")\n",
        "        # Display key evidence factors that might be part of a pattern\n",
        "        key_factors = ['dna_match_strength', 'alibi_strength', 'at_scene_during_murder', \n",
        "                       'had_opportunity', 'fingerprints_at_scene', 'suspicious_behavior',\n",
        "                       'witness_testimony', 'motive_strength', 'prior_conflict', 'time_of_arrival',\n",
        "                       'time_of_departure', 'time_at_scene']\n",
        "        \n",
        "        for factor in key_factors:\n",
        "            if factor in smoking_gun:\n",
        "                print(f\"{factor}: {smoking_gun[factor]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identifying the True Murderer\n",
        "\n",
        "**Detective's Note**: _Our analysis reveals a fascinating contrast between the average and maximum guilt scores. While the average guilt scores point to one suspect, the maximum scores tell a different story. The presence of a specific \"smoking gun\" evidence pattern, which tree-based models excel at detecting, points conclusively to the actual murderer._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Decision Analysis\n",
        "\n",
        "Let's perform a final analysis to conclusively identify our suspect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a final comparison of our top suspects\n",
        "# Let's look at the distributions of guilt scores for each suspect\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot for each model\n",
        "models = ['linear_guilt', 'tree_guilt', 'forest_guilt']\n",
        "model_names = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
        "\n",
        "for i, (model, name) in enumerate(zip(models, model_names)):\n",
        "    plt.subplot(3, 1, i+1)\n",
        "    \n",
        "    # Create a boxplot showing the distribution of guilt scores per suspect\n",
        "    sns.boxplot(x='suspect_name', y=model, data=results)\n",
        "    plt.title(f'Distribution of {name} Guilt Scores by Suspect')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Guilt Score')\n",
        "    if i < 2:\n",
        "        plt.xlabel('')\n",
        "    else:\n",
        "        plt.xlabel('Suspect')\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evidence for Each Prime Suspect\n",
        "\n",
        "Let's take a closer look at the specific evidence against our prime suspects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the specific evidence for the Security Guard Tom Johnson\n",
        "security_guard_data = current_evidence[current_evidence['suspect_name'] == 'Security Guard Tom Johnson']\n",
        "security_guard_results = results[results['suspect_name'] == 'Security Guard Tom Johnson']\n",
        "\n",
        "# Sort the results by forest guilt score to find the most incriminating evidence\n",
        "security_guard_combined = pd.concat([security_guard_data, security_guard_results[['linear_guilt', 'tree_guilt', 'forest_guilt']]], axis=1)\n",
        "security_guard_sorted = security_guard_combined.sort_values('forest_guilt', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Look at the top 3 most incriminating pieces of evidence\n",
        "print(\"Top 3 most incriminating evidence points for Security Guard Tom Johnson:\")\n",
        "for i in range(min(3, len(security_guard_sorted))):\n",
        "    evidence = security_guard_sorted.iloc[i]\n",
        "    print(f\"\\nEvidence #{i+1} - Guilt scores:\")\n",
        "    print(f\"Linear: {evidence['linear_guilt']:.4f}, Tree: {evidence['tree_guilt']:.4f}, Forest: {evidence['forest_guilt']:.4f}\")\n",
        "    \n",
        "    print(\"Key evidence factors:\")\n",
        "    key_factors = ['dna_match_strength', 'alibi_strength', 'at_scene_during_murder', \n",
        "                   'had_opportunity', 'fingerprints_at_scene', 'suspicious_behavior',\n",
        "                   'witness_testimony', 'motive_strength', 'prior_conflict']\n",
        "    \n",
        "    for factor in key_factors:\n",
        "        if factor in evidence:\n",
        "            print(f\"{factor}: {evidence[factor]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's examine the Rival Scientist Dr. Michael Brooks\n",
        "rival_scientist_data = current_evidence[current_evidence['suspect_name'] == 'Rival Scientist Dr. Michael Brooks']\n",
        "rival_scientist_results = results[results['suspect_name'] == 'Rival Scientist Dr. Michael Brooks']\n",
        "\n",
        "# Sort the results by linear guilt score \n",
        "rival_scientist_combined = pd.concat([rival_scientist_data, rival_scientist_results[['linear_guilt', 'tree_guilt', 'forest_guilt']]], axis=1)\n",
        "rival_scientist_sorted = rival_scientist_combined.sort_values('linear_guilt', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Look at the top 3 most incriminating pieces of evidence\n",
        "print(\"Top 3 most incriminating evidence points for Rival Scientist Dr. Michael Brooks:\")\n",
        "for i in range(min(3, len(rival_scientist_sorted))):\n",
        "    evidence = rival_scientist_sorted.iloc[i]\n",
        "    print(f\"\\nEvidence #{i+1} - Guilt scores:\")\n",
        "    print(f\"Linear: {evidence['linear_guilt']:.4f}, Tree: {evidence['tree_guilt']:.4f}, Forest: {evidence['forest_guilt']:.4f}\")\n",
        "    \n",
        "    print(\"Key evidence factors:\")\n",
        "    key_factors = ['dna_match_strength', 'alibi_strength', 'at_scene_during_murder', \n",
        "                   'had_opportunity', 'fingerprints_at_scene', 'suspicious_behavior',\n",
        "                   'witness_testimony', 'motive_strength', 'prior_conflict']\n",
        "    \n",
        "    for factor in key_factors:\n",
        "        if factor in evidence:\n",
        "            print(f\"{factor}: {evidence[factor]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Verdict\n",
        "\n",
        "**Detective's Conclusion:** After thorough analysis of all evidence using multiple modeling approaches, we confidently conclude that **Security Guard Tom Johnson** is the murderer.\n",
        "\n",
        "**Key Findings:**\n",
        "1. While the Rival Scientist shows consistently moderate guilt across all evidence (highest average guilt), Security Guard Tom Johnson has specific evidence points with extremely high guilt scores.\n",
        "2. The random forest model detected a specific pattern of evidence (the \"smoking gun\") for Security Guard Tom Johnson that linear models missed.\n",
        "3. This smoking gun evidence includes high DNA match, low alibi strength, presence at the scene during murder, opportunity, and fingerprints at the scene.\n",
        "4. The linear model was misled by the Rival Scientist's consistent moderate guilt scores across many evidence points, illustrating how linear models focus on average patterns rather than specific combinations of evidence.\n",
        "5. Tree-based models were able to detect the non-linear interactions among evidence factors that conclusively identify Security Guard Tom Johnson as the murderer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case Closed: Final Report\n",
        "Based on your investigation, prepare a final report in the ReadMe.MD File\n",
        "\n",
        "![Detective at Desk](data/assets/6.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
